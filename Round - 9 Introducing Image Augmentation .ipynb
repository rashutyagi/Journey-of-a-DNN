{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Next Important step is Data Transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the list of all the transformations which come pre-built with PyTorch\n",
    "\n",
    "#### Compose\n",
    "#### ToTensor\n",
    "#### ToPILImage\n",
    "#### Normalize\n",
    "#### Resize\n",
    "#### Scale\n",
    "#### CenterCrop\n",
    "#### Pad\n",
    "#### Lambda\n",
    "#### RandomApply\n",
    "#### RandomChoice\n",
    "#### RandomOrder\n",
    "#### RandomCrop\n",
    "#### RandomHorizontalFlip\n",
    "#### RandomVerticalFlip\n",
    "#### RandomResizedCrop\n",
    "#### RandomSizedCrop\n",
    "#### FiveCrop\n",
    "#### TenCrop\n",
    "#### LinearTransformation\n",
    "#### ColorJitter\n",
    "#### RandomRotation\n",
    "#### RandomAffine\n",
    "#### Grayscale\n",
    "#### RandomGrayscale\n",
    "#### RandomPerspective\n",
    "#### RandomErasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will be normalizing our data and also will be converting our data into tensors hence first find out the mean and standard deviation of our data because those are the numbers we will need to supply in the Normalize function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will be applying image augmentation in this time by rotating our images by 5 degrees and that will be done to training data just to make our training data harder and this will be done in transform column only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RASHU TYAGI\\AppData\\Roaming\\Python\\Python37\\site-packages\\torchvision\\datasets\\mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - min: tensor(0.)\n",
      " - max: tensor(1.)\n",
      " - mean: tensor(0.1307)\n",
      " - std: tensor(0.3081)\n",
      " - var: tensor(0.0949)\n"
     ]
    }
   ],
   "source": [
    "# simple transform\n",
    "simple_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                      #  transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
    "                                       # Note the difference between (0.1307) and (0.1307,)\n",
    "                                       ])\n",
    "exp = datasets.MNIST('./data', train=True, download=True, transform=simple_transforms)\n",
    "exp_data = exp.train_data\n",
    "exp_data = exp.transform(exp_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', exp.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', exp.train_data.size())\n",
    "print(' - min:', torch.min(exp_data))\n",
    "print(' - max:', torch.max(exp_data))\n",
    "print(' - mean:', torch.mean(exp_data))\n",
    "print(' - std:', torch.std(exp_data))\n",
    "print(' - var:', torch.var(exp_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean = 0.1307 and Standard Deviation = 0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase transformations\n",
    "train_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.RandomRotation(degrees = (-7.0, 7.0),fill = (1,)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
    "                                       # Note the difference between (0.1307) and (0.1307,)\n",
    "                                       ])\n",
    "\n",
    "# Test Phase transformations\n",
    "test_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
    "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Arguments & Test/Train Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available? True\n"
     ]
    }
   ],
   "source": [
    "SEED = 1 # so that we get same random sample everytime.\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"CUDA Available?\", cuda)\n",
    "\n",
    "# For reproducibility --> so that we get same random sample everytime.\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# dataloader arguments - something you'll fetch these from cmdprmt\n",
    "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
    "\n",
    "# train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will be increasing the capacity of our model which means we will be increasing the number of channels of our model towards the end (after GAP layer).Also we will be using the Dropout layer after every layer because that's the right way to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here in this model we will be performing maxpooling after getting a receptive field of 5*5 only because as we can see that 5 pixels are enough to get the view of the image to an extent that it is recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "dropout_value = 0.1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Input Block\n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 26\n",
    "\n",
    "        # CONVOLUTION BLOCK 1\n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_value)\n",
    "            \n",
    "        ) # output_size = 24\n",
    "        \n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 22\n",
    "\n",
    "        # TRANSITION BLOCK 1\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "         # output_size = 11\n",
    "        self.convblock4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 11\n",
    "        \n",
    "        # CONVOLUTION BLOCK 2\n",
    "        self.convblock5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value)\n",
    "           \n",
    "        ) # output_size = 9\n",
    "        #self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.convblock6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 7\n",
    "\n",
    "        \n",
    "\n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=6)\n",
    "        ) # output_size = 1\n",
    "\n",
    "        # OUTPUT BLOCK\n",
    "        self.convblock7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(dropout_value)\n",
    "       #) # output_size = 7\n",
    "\n",
    "        #self.convblock8 = nn.Sequential(\n",
    "            #nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
    "            # nn.BatchNorm2d(10), NEVER\n",
    "            # nn.ReLU() NEVER!\n",
    "        ) # output_size = 1\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.convblock3(x)\n",
    "        #x = self.pool1(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.convblock4(x)\n",
    "       \n",
    "        x = self.convblock5(x)\n",
    "        #x = self.pool2(x)\n",
    "        x = self.convblock6(x)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        x = self.gap(x) \n",
    "        x = self.convblock7(x)\n",
    "        #x = self.convblock8(x)\n",
    "\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is important to view the model summary and the model parameters and as there is not inbuilt function hence we do it using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\rashu tyagi\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 26, 26]              72\n",
      "              ReLU-2            [-1, 8, 26, 26]               0\n",
      "       BatchNorm2d-3            [-1, 8, 26, 26]              16\n",
      "           Dropout-4            [-1, 8, 26, 26]               0\n",
      "            Conv2d-5           [-1, 16, 24, 24]           1,152\n",
      "              ReLU-6           [-1, 16, 24, 24]               0\n",
      "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
      "           Dropout-8           [-1, 16, 24, 24]               0\n",
      "         MaxPool2d-9           [-1, 16, 12, 12]               0\n",
      "           Conv2d-10           [-1, 16, 10, 10]           2,304\n",
      "             ReLU-11           [-1, 16, 10, 10]               0\n",
      "      BatchNorm2d-12           [-1, 16, 10, 10]              32\n",
      "          Dropout-13           [-1, 16, 10, 10]               0\n",
      "           Conv2d-14             [-1, 32, 8, 8]           4,608\n",
      "             ReLU-15             [-1, 32, 8, 8]               0\n",
      "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
      "          Dropout-17             [-1, 32, 8, 8]               0\n",
      "           Conv2d-18              [-1, 8, 8, 8]             256\n",
      "      BatchNorm2d-19              [-1, 8, 8, 8]              16\n",
      "             ReLU-20              [-1, 8, 8, 8]               0\n",
      "          Dropout-21              [-1, 8, 8, 8]               0\n",
      "           Conv2d-22             [-1, 16, 6, 6]           1,152\n",
      "             ReLU-23             [-1, 16, 6, 6]               0\n",
      "      BatchNorm2d-24             [-1, 16, 6, 6]              32\n",
      "          Dropout-25             [-1, 16, 6, 6]               0\n",
      "        AvgPool2d-26             [-1, 16, 1, 1]               0\n",
      "           Conv2d-27             [-1, 10, 1, 1]             160\n",
      "================================================================\n",
      "Total params: 9,896\n",
      "Trainable params: 9,896\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.61\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "model = Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we can see that we have  13808 parameters lets try for the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  model.train()\n",
    "  pbar = tqdm(train_loader)\n",
    "  correct = 0\n",
    "  processed = 0\n",
    "  for batch_idx, (data, target) in enumerate(pbar):\n",
    "    # get samples\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Init\n",
    "    optimizer.zero_grad()\n",
    "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
    "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model(data)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.nll_loss(y_pred, target)  #nll means negative log loss\n",
    "    train_losses.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update pbar-tqdm\n",
    "    \n",
    "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    processed += len(data)\n",
    "\n",
    "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "    train_acc.append(100*correct/processed)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.1688564270734787 Batch_id=468 Accuracy=83.09: 100%|███████████████████████████| 469/469 [00:07<00:00, 59.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0986, Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.06040928140282631 Batch_id=468 Accuracy=96.87: 100%|██████████████████████████| 469/469 [00:07<00:00, 60.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0538, Accuracy: 9847/10000 (98.47%)\n",
      "\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.13571800291538239 Batch_id=468 Accuracy=97.60: 100%|██████████████████████████| 469/469 [00:07<00:00, 60.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0422, Accuracy: 9884/10000 (98.84%)\n",
      "\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.06797555088996887 Batch_id=468 Accuracy=97.94: 100%|██████████████████████████| 469/469 [00:07<00:00, 59.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0423, Accuracy: 9873/10000 (98.73%)\n",
      "\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.12017613649368286 Batch_id=468 Accuracy=98.16: 100%|██████████████████████████| 469/469 [00:08<00:00, 58.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0403, Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "EPOCH: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.040220096707344055 Batch_id=468 Accuracy=98.33: 100%|█████████████████████████| 469/469 [00:07<00:00, 59.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0299, Accuracy: 9913/10000 (99.13%)\n",
      "\n",
      "EPOCH: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.014273206703364849 Batch_id=468 Accuracy=98.39: 100%|█████████████████████████| 469/469 [00:08<00:00, 58.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0378, Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "EPOCH: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.009669129736721516 Batch_id=468 Accuracy=98.46: 100%|█████████████████████████| 469/469 [00:07<00:00, 59.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0303, Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "EPOCH: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.036205094307661057 Batch_id=468 Accuracy=98.53: 100%|█████████████████████████| 469/469 [00:07<00:00, 59.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9915/10000 (99.15%)\n",
      "\n",
      "EPOCH: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.014077037572860718 Batch_id=468 Accuracy=98.61: 100%|█████████████████████████| 469/469 [00:07<00:00, 60.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "EPOCH: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.048232704401016235 Batch_id=468 Accuracy=98.61: 100%|█████████████████████████| 469/469 [00:08<00:00, 57.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0309, Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "EPOCH: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.023835331201553345 Batch_id=468 Accuracy=98.66: 100%|█████████████████████████| 469/469 [00:07<00:00, 75.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0264, Accuracy: 9921/10000 (99.21%)\n",
      "\n",
      "EPOCH: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.03938356041908264 Batch_id=468 Accuracy=98.77: 100%|██████████████████████████| 469/469 [00:08<00:00, 53.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0249, Accuracy: 9915/10000 (99.15%)\n",
      "\n",
      "EPOCH: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.031098827719688416 Batch_id=468 Accuracy=98.66: 100%|█████████████████████████| 469/469 [00:12<00:00, 36.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0253, Accuracy: 9919/10000 (99.19%)\n",
      "\n",
      "EPOCH: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.04942558333277702 Batch_id=468 Accuracy=98.76: 100%|██████████████████████████| 469/469 [00:08<00:00, 71.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0261, Accuracy: 9921/10000 (99.21%)\n",
      "\n",
      "EPOCH: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.01594415307044983 Batch_id=468 Accuracy=98.80: 100%|██████████████████████████| 469/469 [00:08<00:00, 55.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0225, Accuracy: 9928/10000 (99.28%)\n",
      "\n",
      "EPOCH: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.04095020517706871 Batch_id=468 Accuracy=98.75: 100%|██████████████████████████| 469/469 [00:08<00:00, 73.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0242, Accuracy: 9925/10000 (99.25%)\n",
      "\n",
      "EPOCH: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.0453893207013607 Batch_id=468 Accuracy=98.81: 100%|███████████████████████████| 469/469 [00:08<00:00, 55.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0196, Accuracy: 9941/10000 (99.41%)\n",
      "\n",
      "EPOCH: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.010398929007351398 Batch_id=468 Accuracy=98.80: 100%|█████████████████████████| 469/469 [00:08<00:00, 54.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0225, Accuracy: 9935/10000 (99.35%)\n",
      "\n",
      "EPOCH: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.04404287412762642 Batch_id=468 Accuracy=98.88: 100%|██████████████████████████| 469/469 [00:08<00:00, 55.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 9921/10000 (99.21%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model =  Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results :-\n",
    "\n",
    "### 1.) Best Training Accuracy: 98.88%\n",
    "### 2.) Best Test Accuracy : 99.41%\n",
    "### 3.) Number of parameters : 13,808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Worked perfectly here as we have no overfitting at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we can see a little bit of underfitting because we know that we have made our training data harder hence training accuracy is expected to get low.And we might not have such harder images in test data hence test accuracy is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to Round 10 where we try to make furthur improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
